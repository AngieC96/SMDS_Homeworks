---
title: "Homework_2"
#author: Angela Carraro
date: "29/04/2020"
output:
  rmdformats::readthedown:
  html_document:
    highlight: kate
    lightbox: true
    gallery: true
    toc: yes
    toc_depth: 3
  beamer_presentation:
    highlight: kate
  include: null
  ioslides_presentation:
    highlight: kate
  pdf_document:
    highlight: kate
    keep_tex: yes
    toc: yes
  slide_level: 2
  slidy_presentation:
    fig.height: 3
    fig.width: 4
    highlight: kate
header-includes:
- \usepackage{color}
- \definecolor{Purple}{HTML}{911146}
- \definecolor{Orange}{HTML}{CF4A30}
- \setbeamercolor{alerted text}{fg=Orange}
- \setbeamercolor{frametitle}{bg=Purple}
institute: University of Udine & University of Trieste
graphics: yes
fontsize: 10pt
---
```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.align = 'center', warning=FALSE, message=FALSE, fig.asp=0.625, dev='png', global.par = TRUE, dev.args=list(pointsize=10), fig.path = 'figs/')
```
```{r setup, include=FALSE}
library(knitr)
local({
  hook_plot = knit_hooks$get('plot')
  knit_hooks$set(plot = function(x, options) {
    paste0('\n\n----\n\n', hook_plot(x, options))
  })
})
knitr::opts_chunk$set(echo = TRUE)
```


**Group A: Fernandez Santisteban, Marvulli, Spagnolo, Carraro**

```{r message=FALSE}
library(MASS)
library(DAAG)
```

## DAAG Exercises

Chapter 3 (from page 98), exercises 11, 13. \
Chapter 4 (from page 137), exercises 6, 7.

### Chapter 3, Exercise 11

The following data represent the total number of aberrant crypt foci (abnormal growths in
the colon) observed in seven rats that had been administered a single dose of the carcinogen
azoxymethane and sacrificed after six weeks (thanks to Ranjana Bird, Faculty of Human Ecology,
University of Manitoba for the use of these data):

`87 53 72 90 78 85 83`

Enter these data and compute their sample mean and variance. Is the Poisson model appropriate
for these data? To investigate how the sample variance and sample mean differ under the Poisson
assumption, repeat the following simulation experiment several times:

```{r, eval=FALSE}
x <- rpois(7, 78.3)
mean(x); var(x)
```

**Solution.**

```{r, echo=TRUE}
y <- c(87, 53, 72, 90, 78, 85, 83)
n <- length(y)

#sample mean:
m <- mean(y)
m

#sample variance:
s <- var(y)
s
```

```{r, echo=TRUE}
qreference(test = y, nrep = 8, distribution = function(x) qpois(x,
  lambda = ifelse(is.null(y), 0, 78.3)),
  xlab = "Quantiles of Poisson")
```

Since the pink plots are not really like the blue one, maybe the distribution $\mathcal{Po}(78.3)$ is not the right one to model these data.

Let's investigate further. We take as estimator of the mean the sample mean $\overline{\mu}$ and as estimator of the variance the sample variance $s^2$. Since our assuption is a Poisson distribution, the two values should be equal.

```{r, echo=TRUE}
set.seed(4)
R <- 1000 # number of replications

estimator_mean <- c()
estimator_var <- c()
x <- matrix(NA, R,n)

for (i in 1:R){
  x[i, ] <- rpois(7, 78.3)
  estimator_mean[i] <- mean(x[i, ])
  estimator_var[i] <- var(x[i, ])
}

means <- c(mean(estimator_mean), mean(estimator_var))
means
```

The two values are in fact near each other.

```{r, echo=TRUE}
par(mfrow=c(1,2), xaxt="n")
boxplot(estimator_mean, main="Estimator mean")
abline(h=78.3, lwd=2, col="blue")
par(xaxt="s")
axis(1, 1, expression(hat(mu)))
boxplot(estimator_var, main="Estimator variance")
abline(h=78.3, lwd=2, col="blue")
par(xaxt="s")
axis(1, 1, expression(hat(sigma)^2))
```

The estimator sample mean appears unbiased, while the estimator sample variance is not.

Let’s check now whether all the estimators are consistent. For checking this statement, $n=7$ is extremely low and we need to increse it, let’s say $n=200$.

```{r, echo=TRUE}
estimators_cons <- matrix(NA, R, 2)
n <- 200

for (i in 1:R){
  x <- rpois(n, 78.3)
  estimators_cons[i , 1] <- mean(x)
  estimators_cons[i , 2] <- var(x)
}

variances <- c(var(estimators_cons[, 1]), var(estimators_cons[, 2]))
variances
```

From these values we can see that the efficiency of the sample mean is quite good, since it has a low variance, while the sample variance has a very high variance so it is not so reliable for estimating the parameter $\lambda = 78.3$ of the poisson distribution.

```{r, echo=TRUE}
par(mfrow=c(1,2))
hist(estimators_cons[, 1],  probability = TRUE, 
     breaks=40, main=substitute(hat(mu)), 
     xlab="", cex.main = 1.5)
abline(v=78.3, col="blue", lwd=2)

hist(estimators_cons[, 2],  probability = TRUE, 
     breaks=40, main=substitute(hat(sigma)^2), 
     xlab="", cex.main = 1.5)
abline(v=78.3, col="blue", lwd=2)
```

We can see that the histogram of the sample mean is less wide in width than the histogram of the sample mean. This tells us that the sample mean is the best estimator for the parameter $\lambda$ in a Poisson distribution.

We can see that the sample mean of our sample is basically equal to the mean of the distribution $\mathcal{Po}(78.3)$, and so it is equal to the value of $\lambda$, but the variance of the former is very far from the variance of the latter.

In conclusion, the Poisson model is **not** appropriate for these data, since in a Poisson distribution mean and variance should coincide, while in our sample `y` the sample variance is double the value of the sample mean.


### Chapter 3, Exercise 13

A Markov chain for the weather in a particular season of the year has the transition matrix, from one day to the next:

$$
Pb=
\begin{bmatrix}
 & Sun & Cloud & Rain \\
Sun & 0.6 & 0.2 & 0.2\\
Cloud & 0.2 & 0.4 & 0.4\\
Rain & 0.4 & 0.3 & 0.3
\end{bmatrix}
$$


It can be shown, using linear algebra, that in the long run this Markov chain will visit the states according to the *stationary* distribution:

|Sun | Cloud | Rain|
|---|---|---|
|0.641 | 0.208 | 0.151 |

A result called the *ergodic* theorem allows us to estimate this distribution by simulating the Markov chain for a long enough time.

a. Simulate $1000$ values, and calculate the proportion of times the chain visits each of the states. Compare the proportions given by the simulation with the above theoretical
proportions.

b. Here is code that calculates rolling averages of the proportions over a number of simulations and plots the result. It uses the function $\mathsf{rollmean()}$ from the $zoo$ package.

$$
\begin{align*}
&\mathsf{\text{plotmarkov ←}}\\
&\mathsf{\quad\text{function(n=10000, start=0, window=100, transition=Pb, npanels=5)}}\{\\
&\mathsf{\qquad\text{xc2 ← Markov(n, start, transition)}}\\
&\mathsf{\qquad \text{mav0 ← rollmean(as.integer(xc2==0), window)}}\\
&\mathsf{\qquad \text{mav1 ← rollmean(as.integer(xc2==0), window)}}\\
&\mathsf{\qquad \text{npanel ← cut(1:length(mav0), breaks=seq(from=1, to=length(mav0),}}\\
&\mathsf{\qquad \qquad \qquad  \qquad \text{length=npanels+1), include.lowest=TRUE)}}\\
&\mathsf{\qquad \text{df ← data.frame(av0=mav0, av1=mav1, x=1:length(mav0),
gp=npanel)}}\\
&\mathsf{\qquad \text{print(xyplot(av0+av1 ~ x \mid gp, data=df, layout=c(1,npanels),}}\\
&\mathsf{\qquad \qquad \qquad \qquad \text{type="l", par.strip.text=list(cex=0.65),}}\\
&\mathsf{\qquad \qquad \qquad  \qquad \text{scales=list(x=list(relation="free"))))}}\\
&\}
\end{align*}
$$

Try varying the number of simulations and the width of the window. How wide a window is needed to get a good sense of the stationary distribution? This series settles down rather quickly to its stationary distribution (it "burns in" quite quickly). A reasonable width of window is, however, needed to give an accurate indication of the stationary distribution.

**Solution.**

a. We simulate $1000$ values:

```{r, echo=TRUE}
P=matrix(c(0.6, 0.2, 0.4, 0.2, 0.4, 0.3, 0.2, 0.4,0.3 ), 3, 3)

# The following function simulates N values of a Markov chain sequence,
# with transition matrix P
Markov <- function (N=100, initial.value=1, P)
{
  X <- numeric(N)
  X[1] <- initial.value + 1 
  n <- nrow(P)
  for (i in 2:N){
    X[i] <- sample(1:n, size=1, prob=P[X[i-1], ])}
  X-1
}

#calculation of the proportions
c <- Markov(1000, 2, P)
b <- vector("numeric", 3)
for (i in 0:2){
  b[i+1] <-sum(c==i)/length(c)}
b
```

Repeating the simulation, in general the first proportion is lower, while the other two proportions are higher than the theoretical ones. \
This may happen because we are not considering such a long period.

b. We try varying the number of simulations and the width of the window:

```{r}
library(zoo)
library(lattice)

plotmarkov <- function(n=10000, start=0, window=100, transition=P, npanels=5){
    xc2 <- Markov(n, start, transition)
    mav0 <- rollmean(as.integer(xc2==0), window)
    mav1 <- rollmean(as.integer(xc2==0), window)
    npanel <- cut(1:length(mav0), breaks=seq(from=1, to=length(mav0),
                 length=npanels+1), include.lowest=TRUE)
    df <- data.frame(av0=mav0, av1=mav1, x=1:length(mav0), gp=npanel)
    print(xyplot(av0+av1 ~ x | gp, data=df, layout=c(1,npanels),
                 type="l", par.strip.text=list(cex=0.65),
                 scales=list(x=list(relation="free"))))
}
plotmarkov()
```

```{r}
plotmarkov(n=1000000, window=10000)
```




### Chapter 4, Exercise 6

Here we generate random normal numbers with a sequential dependence structure:

$$
\begin{align*}
&\mathsf{\text{y1 ← rnorm(51)}}\\
&\mathsf{\text{y ← y1[-1] + y1[-51]}}\\
&\mathsf{\text{acf(y1)              # acf is ‘autocorrelation function’}}\\
&\mathsf{\text{                       # (see Chapter 9)}}\\
&\mathsf{\text{acf(y)}}
\end{align*}
$$

Repeat this several times. There should be no consistent pattern in the $\mathsf{acf}$ plot for different random samples $\mathsf{y1}$. There will be a fairly consistent pattern in the $\mathsf{acf}$ plot for $\mathsf{y}$, a result of the correlation that is introduced by adding to each value the next value in the sequence.

**Solution.**

```{r, echo=TRUE}
par(mfrow=c(4, 2), mar=c(3,2.5,1,1), mgp=c(1.5, 0.5, 0))
for(i in 1:4) {
  y1 <- rnorm(51)
  y <- y1[-1] + y1[-51]
  acf(y1, ylab = "y1")
  acf(y, ylab = "y")
}
```

We can see that in the $\mathsf{acf}$ plot for $\mathsf{y1}$ there is no correlation: in the plots the lineas are always under the horizontal blue lines. Instead,  in the $\mathsf{acf}$ plot for $\mathsf{y}$ the values sometimes are above or really near the blue lines.

### Chapter 4, Exercise 7

Create a function that does the calculations in the first two lines of the previous exercise:

```{r, eval=FALSE}
y1 <- rnorm(51)
y <- y1[-1] + y1[-51]
```


Put the calculation in a loop that repeats 25 times. Calculate the mean and variance for each vector $\mathsf{y}$ that is returned. Store the 25 means in the vector $\mathsf{av}$, and store the 25 variances in
the vector $\mathsf{v}$. Calculate the variance of $\mathsf{av}$.

**Solution.**

The base implementation of the function is very easy. The point is that this function will be applied several times (passing its input from a matrix). In order to avoid unnecessary loops, before use the function it is neccessary to vectorize it. Vectorized function has been saved with the name $\mathsf{norm\_sample\_general(n)}$ where $\mathsf{n}$ is the number of samples to generate. It is run by setting 51 samples and 25 repetitions and then results are saved on $\mathsf{y}$ matrix.
```{r DAAG4.7_1, echo=TRUE}
#function to get n samples
norm_sample <- function(n) {
  y <- rnorm(n);
  y <- y[-1] + y[-n];
  return(y)
}

#vectorization of the previous function
norm_sample_general <- Vectorize(norm_sample)

#get 25 sets of 51 (50) samples
y <- norm_sample_general(rep(51, 25))
y[1:5,1:5]
```

Now mean and variance for each column of $\mathsf{y}$ must be computed. In the case of the mean it can be easily computed by applying the function $\mathsf{colMeans()}$ directly to $\mathsf{y}$. However, there is not any function similar to the previous one to calculate the variance (in the $\texttt{R}$ base package). A similar function has been implemented with the name $\mathsf{colVars()}$.
```{r DAAG4.7_2, echo=TRUE}
#colVars function
colVars <- function(x) {
  return((colSums(x^2) - colSums(x))/length(x[1,]))
}

#mean and variance of all repetitions
av <- colMeans(y)
v <- colVars(y)

#first 5 means
av[1:5]

#first 5 variances
v[1:5]
```

Finally it is computed the variance of the means.

```{r DAAG4.7_3, echo=TRUE}
#variance of means
variance <- var(av)
variance
```



## CS Exercises

Chapter 3 (from page 76), exercises 3.3 (hint: use system.time() function), 3.5.

### Exercise 3.3

Rewrite the following, replacing the loop with efficient code:

```{r, eval=FALSE, include=TRUE}
n <- 100000; z <- rnorm(n)
zneg <- 0; j <- 1
for (i in 1:n) {
  if (z[i]<0) {
    zneg[j] <- z[i]
    j <- j + 1
  }
}
```

Confirm that your rewrite is faster but gives the same result.

**Solution.**

A more efficient code is:

```{r, echo=TRUE}
n <- 100000; z <- rnorm(n)

start_time2 <- Sys.time()
zneg2 <- z[which(z < 0)]
end_time2 <- Sys.time()
end_time2 - start_time2

system.time(zneg2 <- z[which(z < 0)])
```

While the time of the given code is:

```{r}
start_time <- Sys.time()
zneg <- 0; j <- 1
for (i in 1:n) {
  if (z[i]<0) {
    zneg[j] <- z[i]
    j <- j + 1
  }
}
end_time <- Sys.time()
end_time - start_time

system.time({zneg <- 0; j <- 1
for (i in 1:n) {
  if (z[i]<0) {
    zneg[j] <- z[i]
    j <- j + 1
  }
}})
```

We can see that our code is about one order of magintude faster that the given code.

We have that the two methods give the same result:

```{r, echo=TRUE}
identical(zneg2, zneg)
```


## Exercise 3.5

Consider solving the matrix equation $\mathbf{Ax}=\mathbf{y}$ for $\mathbf{x}$, where $\mathbf{y}$ is a known $n \times n$ matrix. The formal solution to the problem is $\mathbf{x}= \mathbf{A}^{-1} \mathbf{y}$, but it is possible to solve the equation directly, without actually forming $\mathbf{A}^{-1}$. This question explores this direct solution. Read the help file for `solve` before trying it.

a. First create and $\mathbf{A}$, $\mathbf{x}$ and $\mathbf{y}$ satisfying $\mathbf{Ax}=\mathbf{y}$.

```{r, eval=FALSE}
set.seed(0); n <- 1000
A <- matrix(runif(n*n),n,n); x.true <- runif(n)
y <- A%*%x.true
```

The idea is to experiment with solving $\mathbf{Ax}=\mathbf{y}$ for $\mathbf{x}$, but with a known truth to compare the answer to.

b. Using `solve`, form the matrix $\mathbf{A}^{-1}$ explicitly and then form $\mathbf{x}_1= \mathbf{A}^{-1} \mathbf{y}$. Note how long this takes. Also assess the mean absolute difference between `x1` and `x.true` (the approximate mean absolute 'error' in the solution).

c. Now use `solve` to directly solve for $\mathbf{x}$ without forming $\mathbf{A}^{-1}$. Note how long this takes and assess the mean absolute error of the result.

d. What do you conclude?


**Solution**

a. We create and $\mathbf{A}$, $\mathbf{x}$ and $\mathbf{y}$ satisfying $\mathbf{Ax}=\mathbf{y}$:

```{r}
set.seed(0); n <- 1000
A <- matrix(runif(n*n), n, n);
x.true <- runif(n)
y <- A %*% x.true  
#solve(A, y)  #cerco x che risolva l'equazione
```

b. Let's solve the inverse by using the function `solve(A)` and let's compute how long it takes throught the function `system.time`.

```{r}
set.seed(0); n <- 1000
A <- matrix(runif(n*n),n,n); x.true <- runif(n)
x.true <- runif(n)
Ainv <- solve(A) 
x1 <- Ainv %*% y
mad(x.true, x1)
system.time(solve(A) %*% y, gcFirst= TRUE)
```

c. We use `solve` to directly solve for $\mathbf{x}$ without forming $\mathbf{A}^{-1}$:

```{r}
set.seed(0); n <- 1000
A <- matrix(runif(n*n), n, n);
x.true <- runif(n)
x2 <- solve(A, y)
mad(x.true, x2)
system.time(solve(A, y), gcFirst=TRUE)
```

d. Through the way exposed in point (b) we get the median absolute error 2.577135e-11, while through the way exposed in point (c) we get the median absolute error 1.708318e-12, less than the previous one; moreover, this last solving procedure takes less than the other one in terms of computational cost (as the result of `system.time` shows). Hence we get that the method of solving $\mathbf{Ax}=\mathbf{y}$ which does not involve the computation of the inverse matrix is more efficient.



## LAB Exercises

Exercises 1, 2, 3, 4 and 5.

### Exercise 1

Check the biased nature of $s^2_b = \frac1n \sum^n_{i=1} (y_i - \bar{y})^2$ via MC simulation, generating $n=10$ iid values from a normal distribution. Plot also $s^2 = \frac1{n-1} \sum^n_{i=1} (y_i - \bar{y})^2$ and comment the difference.

**Solution.**

```{r, echo=TRUE}
set.seed(123)
R <- 1000
n <- 10
#generate n values R times
samples  <- array(0, c(R, n))
for (i in 1:R)
  samples[i, ] <- rnorm(n, 0, 1)

#sample variance
v <- apply(samples, 1,  var)

sigma <- 1
par(mfrow=c(1,1), oma=c(0,0,0,0))
hist(v, breaks = 40, probability = TRUE,  ylim = c(0, 1.1), main= bquote(s^2), cex.main=1.5)
curve((n/sigma^2) * dchisq(x * (n/sigma^2), df = n - 1),
      add = TRUE, col = "blue", lwd = 2)
```

We can see that $s^2_b$ is a biased estimator for the variance because its mean is lower than 1, in fact

```{r}
m <- mean(v)
m
var_b <- c()
for(i in 1:R) {
  var_b[i] <- sum(samples[i, ])**2/n
}
m_b <- mean(var_b)
m_b
```


We compare $s_b^2$ and $s^2$:

```{r, echo=TRUE}
par (mfrow=c(1,1), oma=c(0,0,0,0))
hist(v, breaks= 40, probability = TRUE,  ylim = c(0, 1.1),  main= "Comparison", cex.main=1.5)
curve((n/sigma^2) * dchisq(x * (n/sigma^2), df = n - 1),
      add = TRUE, col="blue", lwd=2)
abline(v=m_b, col="blue")
curve(((n-1)/sigma^2) * dchisq(x * ((n-1)/sigma^2), df = n - 1),
      add = TRUE, col="red", lwd=2)
abline(v=m, col="red")
legend("topright", legend = c(expression("s^2"), expression("s^2_b")), col = c("red", "blue"), lty = 1)
```
We compare the two estimators: $s^2$, being unbiased, can be preferred.


### Exercise 2

What happens if a great player decides to join you, now? Try to simulate the data and perform the test again.

**Solution.** (Dovr? rivederla (ROSS))

If the great player plays alone we have:

```{r}
library("ggplot2")
set.seed(101)
n <- 50
K <- 4

# generate the values
y <- sample(1:K, n, replace=TRUE, prob = c(1/16, 3/16, 5/16, 7/16))
observed <- table(y)
expected <- c(n*(1/16), n*(3/16), n*(5/16), n*(7/16))
x2 <- sum((observed-expected)^(2)/expected)
# manually compute the p-value
pchisq(x2, df =K-1, lower.tail =FALSE )
# same result with the chisq.test function
chisq.test(observed, p = c( 1/16, 3/16, 5/16, 7/16))
```

If he plays with other 6 friends, such that the total numeber of players is 7 , we have:

```{r}
M <- 7
friends <- array(0, c(M, n))
# 6 palyers are bad players
for (j in (1:(M-1)))
  friends[j, ] <- sample(1:K, n, replace=TRUE, prob=c(7/16, 5/16, 3/16, 1/16))
# here the pro player
friends[M, ] <- sample(1:K, n, replace=TRUE, prob=c(1/16, 3/16, 5/16, 7/16))

############
observed <- array(0, c(M, K))
for (j in (1:M))
{
  observed[j, ] <- table(friends[j, ]) 
}

###########
expected <- array(0, c(M, K))
for (j in (1:(M-1)))
  expected[j, ] <- c( n*(7/16), n*(5/16), n*(3/16), n*(1/16) )

expected[M, ] <- c( n*(1/16), n*(3/16), n*(5/16), n*(7/16) )

##########
difference <- array(0, c(M, K))
for (j in (1:M))
{
  for (i in (1:K))
  {
    difference[j, i] = ((observed[j, i] - expected[j, i])^2/expected[j, i])
  }
}

x2 <- sum(difference)
pchisq(x2, df = K-1, lower.tail =FALSE )
```


### Exercise 3

Sometimes it could be useful to asses the degree of association, or correlation, between paired samples, using the Pearson, the Kendall's $\tau$ or the Spearman's $\rho$ correlation coefficient. Regardless of the adopted cofficient, the null hypothesis for a given correlation coefficent $\rho$ is:

$$
H_0: \rho = 0
$$
The test statistic is then defined as

$$
T = r \sqrt{\frac{n-2}{1-r^2}} \underset{H_0}{\sim} t_{n-2},
$$
where $\mathrm{Corr}(X, Y)$ is the Pearson correlation coefficient. Suppose to have two samples of the same length $x_1, \ldots, x_n$, $y_1, \ldots, y_n$, and to measure the association between them. Once we compute the test statistic $t_\mathrm{obs}$, we may then compute the $p$-value (here we are evaluating a two sided test) as:

$$
p = 2 Pr_{H_0}(T \geq |T_\mathrm{obs}|).
$$
Consider now some of the most followed Instagram accounts in 2018: for each of the owners, we report also the number of Twitter followers (in milions). Are the Instagram and Twitter account somehow associated? Perform a correlation test, compute the $p$-value and give an answer. Here is the dataframe.

```{r lab3, echo=TRUE}
Owners <- c( "Katy Perry", "Justin Bieber", "Taylor Swift", "Cristiano Ronaldo",
                   "Kim Kardashian", "Ariana Grande", "Selena Gomez", "Demi Lovato")
Instagram <- c( 69, 98,107, 123, 110, 118, 135, 67)
Twitter <- c( 109, 106, 86, 72, 59, 57, 56, 56)
plot( Instagram, Twitter, pch=21, bg=2, xlim=c(60, 150), ylim=c(40, 120) )
text( Instagram[-6], Twitter[-6]+5, Owners[-6], cex=0.8 )
text( Instagram[6], Twitter[6]-5, Owners[6], cex=0.8 )
```

**Solution.**

In order to perform the test, let's calculate the $p$-value corresponding to the observations. This $p$-value will be calculated manually by using the Pearson's correlation.

```{r lab3_sol, echo=TRUE}
#parameters and correlation
r <- cor(x=Instagram, y=Twitter, method = "pearson")
n <- length(Instagram)
alpha <- 0.05

#t statistic and p-value
t <- r*sqrt((n-2)/(1-r*r))
pvalue <- pt(t, df=n-2, lower.tail = TRUE)

pvalue
```

The resulting $p$-value is greater than $0.05$, so null hypothesis, in this case $\rho=0$, should be accepted. It means that in terms of the test, there is no sufficient significance to assume any association between the number of followers on Twitter and Instagram. Note that it does not implies that this correlation does not exist, it simply establish the fact that, according to the test, it cannot be confirmed that it exists any correlation, also if it is the case.

The situation can also be shown on a graph:

```{r lab3_plot, echo=TRUE}
#plot
library(RColorBrewer)
plotclr <- brewer.pal(6,"YlOrRd")
curve(dt(x,n-2),xlim=c(-5,5), ylim=c(0,0.4), main="p-values and rejection region",
      col = "blue", lwd = 2, xlab="Corr(X, Y)",  ylab=expression(t[n-2]),  yaxs="i")
cord.x_r <- c(qt(1-alpha/2,n-2),seq(qt(1-alpha/2,n-2), 5, 0.01), 5)
cord.y_r <- dt(cord.x_r, n-2); cord.y_r[1] <- 0; cord.y_r[length(cord.y_r)] <- 0
cord.x_l <- c(-5, seq(-5, qt(alpha/2,n-2), 0.01), qt(alpha/2,n-2))
cord.y_l <- dt(cord.x_l, n-2); cord.y_l[1] <- 0; cord.y_l[length(cord.y_l)] <- 0
polygon(cord.x_r,cord.y_r,col=plotclr[3], border = NA )
polygon(cord.x_l,cord.y_l,col=plotclr[3], border = NA )
curve(dt(x,n-2),xlim=c(-5,5),main=expression(t[n-2]), col = "blue", lwd = 2, add = TRUE, yaxs="i")
abline(v =t, lty=2, lwd=2, col="red")
text (0,0.2, paste("Accept", expression(H0)))
text (4.3,0.08, paste("Reject", expression(H0)))
text (-4.3,0.08, paste("Reject", expression(H0)))
text(as.double(t)-0.15, 0.02, "t", col="red", cex=1.2)
```

As can be seen, the estimated statistic $T$ is within the acceptance region, as $p$-value indicates.


### Exercise 4

Compute analitically $J(\gamma, \gamma; y)$, $J(\gamma, \beta; y)$, $J(\beta, \beta; y)$.

**Solution.**

Since $J$ matrix must be derived from the log-likelihood, let's remember its expression:

$$
l(\gamma,\beta;y)=n \log(\gamma)  -  n \gamma \log(\beta)  +  \gamma \sum_{i=1}^n(\log(y_i))  -  \sum_{i=1}^n(y_i/\beta)^\gamma
$$
As $J$ matrix is defined by using the second-order derivatives on model's parameters, previously first-order derivatives must be computed. In order to make the computation, it will be consiedered the next identities:

$$
\begin{cases}
\sum_{i=1}^n(y_i/\beta)^\gamma = \beta^{-\gamma}\sum_{i=1}^ny_i^\gamma \ \ \ \ \ \ \mathrm{(a)} \\
\sum_{i=1}^n(y_i/\beta)^\gamma = \sum_{i=1}^ne^{\gamma\log(y_i/\beta)} \ \ \mathrm{(b)}
\end{cases}
$$

In the computation of $\partial_\gamma$ and $\partial_\beta$, (b) and (a) identities are used respectively.

$$
\frac{\partial l(\gamma,\beta;y)}{\partial \gamma}  =  \frac{n}{\gamma}  -  n\log(\beta)  +  \sum_{i=1}^n\log(y_i/\beta)^\gamma   \\
\frac{\partial l(\gamma,\beta;y)}{\partial \beta} = \frac{n\gamma}{\beta}  -  \frac{\gamma}{\beta} \sum_{i=1}^ny^i/\beta
$$
Now it is possible to compute second-order derivatives, which constitutes the elements of the $J$ matrix. Again to compute this expressions the (a) and (b) identities are used.

1. $\partial_{\gamma\gamma}^2$ term:

$$
\frac{\partial^2 l(\gamma,\beta;y)}{\partial\gamma^2}  =  -\frac{n}{\gamma^2}  +  \sum_{i=1}^n \left( \frac{y_i}{\beta} \right)^\gamma \log\left( \frac{y_i}{\beta} \right)^2
$$
2. $\partial_{\gamma\beta}^2$ term:

$$
\frac{\partial^2 l(\gamma,\beta;y)}{\partial\gamma\partial\beta}  =  \frac{1}{\beta} \left[ n  +  \sum_{i=1}^n \left( \frac{y_i}{\beta} \right)^\gamma  +  \gamma\sum_{i=1}^n \left( \frac{y_i}{\beta} \right)^\gamma \log \frac{y_i}{\beta} \right]
$$
3. $\partial_{\beta\gamma}^2$ term:

$$
\frac{\partial^2 l(\gamma,\beta;y)}{\partial\beta\partial\gamma}  =  -\frac{n}{\beta}  +  \sum_{i=1}^n \left( \frac{y_i}{\beta} \right)^{\gamma-1}  -  \frac{\gamma}{\beta}\sum_{i=1}^n \left( \frac{y_i}{\beta} \right)^\gamma \log \frac{y_i}{\beta}
$$
4. $\partial_{\beta\beta}^2$ term:

$$
\frac{\partial^2 l(\gamma,\beta;y)}{\partial\beta^2}  =  -\frac{n\gamma}{\beta^2}  +  \gamma(\gamma+1)\beta^{-\gamma-2} \sum_{i=1}^ny_i^\gamma
$$


### Exercise 5

Produce the contour plot for the quadratic approximation of the log-likelihood, based on the Taylor series:

$$
\mathcal{l}(\theta) − \mathcal{l}(\widehat{\theta}) \approx - \frac12 (\theta - \widehat{\theta})^T J(\widehat{\theta})(\theta - \widehat{\theta}).
$$

**Solution.**

We then have that

$$
\mathcal{l}(\theta) \approx \mathcal{l}(\widehat{\theta}) - \frac12 (\theta - \widehat{\theta})^T J(\widehat{\theta})(\theta - \widehat{\theta}).
$$

We write the log-likelihood function in $\texttt{R}$:

```{r, echo=TRUE}
log_lik_weibull <- function(data, param){
  sum(dweibull(data, shape = param[1], scale = param[2], log = TRUE))
}
```

Then we write the quadratic approximation of the log-likelihood function in $\texttt{R}$:

```{r}
qapprox_llweibull <- function(data, param, paramhat, Jhat){
  diff <- cbind(param[1] - paramhat[1], param[2] - paramhat[2])
  log_lik_weibull(data, paramhat) - 0.5 * diff %*% Jhat %*% t(diff)
}
```

Finally, we show the contour plot:

```{r, echo=TRUE}
y <- c(155.9, 200.2, 143.8, 150.1,152.1, 142.2, 147, 146, 146, 170.3, 148, 140, 118, 144, 97)
n <- length(y)

gammahat<-uniroot(function(x) n/x+sum(log(y))-n*sum(y^x*log(y))/sum(y^x), c(1e-5,15))$root
betahat<- mean(y^gammahat)^(1/gammahat)
weib.y.mle<-c(gammahat,betahat) #first element is the MLE for the shape gamma,
# while second element is the MLE for the scale beta

#observed information matrix
jhat<-matrix(NA,nrow=2,ncol=2)
jhat[1,1]<-n/gammahat^2+sum((y/betahat)^gammahat* (log(y/betahat))^2)
jhat[1,2]<-jhat[2,1]<- n/betahat-sum(y^gammahat/betahat^(gammahat+1)*(gammahat*log(y/betahat)+1))
jhat[2,2]<- -n*gammahat/betahat^2+gammahat*(gammahat+1) / betahat^(gammahat+2)*sum(y^gammahat)

#define parameters grid
gamma <- seq(0.1, 15, length=100)
beta <- seq(100,200, length=100)
parvalues <- expand.grid(gamma,beta)
appllikvalues <- apply(parvalues, 1, qapprox_llweibull, data=y,
                    paramhat=weib.y.mle, Jhat=jhat)
appllikvalues <- matrix(appllikvalues, nrow=length(gamma),
                     ncol=length(beta), byrow=F)
conf.levels <- c(0,0.5,0.75,0.9,0.95,0.99)

#contour plot
contour(gamma, beta, appllikvalues - max(appllikvalues),
    levels=-qchisq(conf.levels, 2)/2,
    xlab=expression(gamma),
    labels=as.character(conf.levels),
    ylab=expression(beta))
title('Weibull quadratic approximation of log likelihood')

#image
image(gamma, beta, appllikvalues - max(appllikvalues), zlim=c(-6,0),
    col=terrain.colors(20),xlab=expression(gamma),
    ylab=expression(beta))
title('Weibull quadratic approximation of log likelihood')
```


<!-- knitr::knit("Homework1_2020_GROUP_I.Rmd", tangle = TRUE, output ="Homework1_2020_GROUP_I.R") -->