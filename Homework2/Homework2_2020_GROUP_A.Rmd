
---
title: "Homework_2"
#author: Angela Carraro
date: "29/04/2020"
output:
  rmdformats::readthedown:
  html_document:
    highlight: kate
    lightbox: true
    gallery: true
    toc: yes
    toc_depth: 3
  beamer_presentation:
    highlight: kate
  include: null
  ioslides_presentation:
    highlight: kate
  pdf_document:
    highlight: kate
    keep_tex: yes
    toc: yes
  slide_level: 2
  slidy_presentation:
    fig.height: 3
    fig.width: 4
    highlight: kate
header-includes:
- \usepackage{color}
- \definecolor{Purple}{HTML}{911146}
- \definecolor{Orange}{HTML}{CF4A30}
- \setbeamercolor{alerted text}{fg=Orange}
- \setbeamercolor{frametitle}{bg=Purple}
institute: University of Udine & University of Trieste
graphics: yes
fontsize: 10pt
---
```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.align = 'center', warning=FALSE, message=FALSE, fig.asp=0.625, dev='png', global.par = TRUE, dev.args=list(pointsize=10), fig.path = 'figs/')
```
```{r setup, include=FALSE}
library(knitr)
local({
  hook_plot = knit_hooks$get('plot')
  knit_hooks$set(plot = function(x, options) {
    paste0('\n\n----\n\n', hook_plot(x, options))
  })
})
knitr::opts_chunk$set(echo = TRUE)
```


**Group A: Fernandez Santisteban, Marvulli, Spagnolo, Carraro**

```{r message=FALSE}
library(MASS)
library(DAAG)
```

## DAAG Exercises

Chapter 3 (from page 98), exercises 11, 13. \
Chapter 4 (from page 137), exercises 6, 7.

### Chapter 3, Exercise 11

The following data represent the total number of aberrant crypt foci (abnormal growths in
the colon) observed in seven rats that had been administered a single dose of the carcinogen
azoxymethane and sacrificed after six weeks (thanks to Ranjana Bird, Faculty of Human Ecology,
University of Manitoba for the use of these data):

`87 53 72 90 78 85 83`

Enter these data and compute their sample mean and variance. Is the Poisson model appropriate
for these data? To investigate how the sample variance and sample mean differ under the Poisson
assumption, repeat the following simulation experiment several times:

```{r, eval=FALSE}
x <- rpois(7, 78.3)
mean(x); var(x)
```

**Solution.**

```{r, echo=TRUE}
y <- c(87, 53, 72, 90, 78, 85, 83)
n <- length(y)

#sample mean:
m <- mean(y)
m

#sample variance:
s <- var(y)
s
```

```{r, echo=TRUE}
qreference(test = y, nrep = 8, distribution = function(x) qpois(x,
  lambda = ifelse(is.null(y), 0, 78.3)),
  xlab = "Quantiles of Poisson")
```

Since the pink plots are not really like the blue one, maybe the distribution $\mathcal{Po}(78.3)$ is not the right one to model these data.

Let's investigate further. We take as estimator of the mean the sample mean $\overline{\mu}$ and as estimator of the variance the sample variance $s^2$. Since our assuption is a Poisson distribution, the two values should be equal.

```{r, echo=TRUE}
set.seed(4)
R <- 1000 # number of replications

estimator_mean <- c()
estimator_var <- c()
x <- matrix(NA, R,n)

for (i in 1:R){
  x[i, ] <- rpois(7, 78.3)
  estimator_mean[i] <- mean(x[i, ])
  estimator_var[i] <- var(x[i, ])
}

means <- c(mean(estimator_mean), mean(estimator_var))
means
```

The two values are in fact near each other.

```{r, echo=TRUE}
par(mfrow=c(1,2), xaxt="n")
boxplot(estimator_mean, main="Estimator mean")
abline(h=78.3, lwd=2, col="blue")
par(xaxt="s")
axis(1, 1, expression(hat(mu)))
boxplot(estimator_var, main="Estimator variance")
abline(h=78.3, lwd=2, col="blue")
par(xaxt="s")
axis(1, 1, expression(hat(sigma)^2))
```

The estimator sample mean appears unbiased, while the estimator sample variance is not.

```{r, echo=TRUE}
variances <- c(var(estimator_mean), var(estimator_var))
variances
```

From these values we can see that the efficiency of the sample mean is quite good, since it has a low variance, while the sample variance has a very high variance so it is not reliable for estimating the parameter $\lambda = 78.3$ of the poisson distribution.

Let’s check now whether all the estimators are consistent. For checking this statement, $n=7$ is extremely low and we need to increse it, let’s say $n=200$.

```{r, echo=TRUE}
estimators_cons <- matrix(NA, R, 2)
n <- 200

for (i in 1:R){
  x <- rpois(n, 78.3)
  estimators_cons[i , 1] <- mean(x)
  estimators_cons[i , 2] <- var(x)
}

par(mfrow=c(1,2))
hist(estimators_cons[, 1],  probability = TRUE, 
     breaks=40, main=substitute(hat(mu)), 
     xlab="", cex.main = 1.5)
abline(v=78.3, col="blue", lwd=2)

hist(estimators_cons[, 2],  probability = TRUE, 
     breaks=40, main=substitute(hat(sigma)^2), 
     xlab="", cex.main = 1.5)
abline(v=78.3, col="blue", lwd=2)
```

We can see that the histogram of the sample mean is less wide in width than the histogram of the sample mean. This tells us that the sample mean is the best estimator for the parameter $\lambda$ in a Poisson distribution.

We can see that the sample mean of our sample is basically equal to the mean of the distribution $\mathcal{Po}(78.3)$, but the variance of the former is very far from the variance of the latter.

In conclusion, the Poisson model is **not** appropriate for these data, since in a Poisson distribution mean and variance should coincide, while in our sample `y` the sample variance is double the value of the sample mean.


### Chapter 4, Exercise 7

Create a function that does the calculations in the first two lines of the previous exercise. Put the calculation in a loop that repeats 25 times. Calculate the mean and variance for each vector $\mathsf{y}$ that is returned. Store the 25 means in the vector $\mathsf{av}$, and store the 25 variances in
the vector $\mathsf{v}$. Calculate the variance of $\mathsf{av}$.

**Solution.**

The base implementation of the function is very easy. The point is that this function will be applied several times (passing its input from a matrix). In order to avoid unnecessary loops, before use the function it is neccessary to vectorize it. Vectorized function has been saved with the name $\mathsf{norm\_sample\_general(n)}$ where $\mathsf{n}$ is the number of samples to generate. It is run by setting 51 samples and 25 repetitions and then results are saved on $\mathsf{y}$ matrix.
```{r DAAG4.7_1, echo=TRUE}
#function to get n samples
norm_sample <- function(n) {
  y <- rnorm(n);
  y <- y[-1] + y[-n];
  return(y)
}

#vectorization of the previous function
norm_sample_general <- Vectorize(norm_sample)

#get 25 sets of 51 (50) samples
y <- norm_sample_general(rep(51, 25))
y[1:5,1:5]
```

Now mean and variance for each column of $\mathsf{y}$ must be computed. In the case of the mean it can be easily computed by applying the function $\mathsf{colMeans()}$ directly to $\mathsf{y}$. However, there is not any function similar to the previous one to calculate the variance (in the $\texttt{R}$ base package). A similar function has been implemented with the name $\mathsf{colVars()}$.
```{r DAAG4.7_2, echo=TRUE}
#colVars function
colVars <- function(x) {
  return((colSums(x^2) - colSums(x))/length(x[1,]))
}

#mean and variance of all repetitions
av <- colMeans(y)
v <- colVars(y)

#first 5 means
av[1:5]

#first 5 variances
v[1:5]
```

Finally it is computed the variance of the means.

```{r DAAG4.7_3, echo=TRUE}
#variance of means
variance <- var(av)
variance
```



## CS Exercises

Chapter 3 (from page 76), exercises 3.3 (hint: use system.time() function), 3.5.

### Exercise 3.3

Rewrite the following, replacing the loop with efficient code:

```{r, eval=FALSE, include=TRUE}
n <- 100000; z <- rnorm(n)
zneg <- 0; j <- 1
for (i in 1:n) {
  if (z[i]<0) {
    zneg[j] <- z[i]
    j <- j + 1
  }
}
```

Confirm that your rewrite is faster but gives the same result.

**Solution.**

A more efficient code is:

```{r, echo=TRUE}
n <- 100000; z <- rnorm(n)

start_time2 <- Sys.time()
zneg2 <- z[which(z < 0)]
end_time2 <- Sys.time()

end_time2 - start_time2
```

While the time of the given code is:

```{r}
start_time <- Sys.time()
zneg <- 0; j <- 1
for (i in 1:n) {
  if (z[i]<0) {
    zneg[j] <- z[i]
    j <- j + 1
  }
}

end_time <- Sys.time()

end_time - start_time
```

We can see that our code is about 10 times faster that the given code.

We have that the two methods give the same result:

```{r, echo=TRUE}
identical(zneg2, zneg)
```



## LAB Exercises

Exercises 1, 2, 3, 4 and 5.

### Exercise 1

Check the biased nature of $s^2_b$ via MC simulation, generating $n=10$ iid values from a normal distribution. Plot also $s^2$ and comment the difference.

**Solution.**



### Exercise 2

What happens if a great player decides to join you, now? Try to simulate the data and perform the test again.

**Solution.**


### Exercise 3

Sometimes it could be useful to asses the degree of association, or correlation, between paired samples, using the Pearson, the Kendall's $\tau$ or the Spearman's $\rho$ correlation coefficient. Regardless of the adopted cofficient, the null hypothesis for a given correlation coefficent $\rho$ is:

$$
H_0: \rho = 0
$$
The test statistic is then defined as

$$
T = r \sqrt{\frac{n-2}{1-r^2}} \underset{H_0}{\sim} t_{n-2},
$$
where $\mathrm{Corr}(X, Y)$ is the Pearson correlation coefficient. Suppose to have two samples of the same length $x_1, \ldots, x_n$, $y_1, \ldots, y_n$, and to measure the association between them. Once we compute the test statistic $t_\mathrm{obs}$, we may then compute the $p$-value (here we are evaluating a two sided test) as:

$$
p = 2 Pr_{H_0}(T \geq |T_\mathrm{obs}|).
$$
Consider now some of the most followed Instagram accounts in 2018: for each of the owners, we report also the number of Twitter followers (in milions). Are the Instagram and Twitter account somehow associated? Perform a correlation test, compute the $p$-value and give an answer. Here is the dataframe.

```{r lab3, echo=TRUE}
Owners <- c( "Katy Perry", "Justin Bieber", "Taylor Swift", "Cristiano Ronaldo",
                   "Kim Kardashian", "Ariana Grande", "Selena Gomez", "Demi Lovato")
Instagram <- c( 69, 98,107, 123, 110, 118, 135, 67)
Twitter <- c( 109, 106, 86, 72, 59, 57, 56, 56)
plot( Instagram, Twitter, pch=21, bg=2, xlim=c(60, 150), ylim=c(40, 120) )
text( Instagram[-6], Twitter[-6]+5, Owners[-6], cex=0.8 )
text( Instagram[6], Twitter[6]-5, Owners[6], cex=0.8 )
```

**Solution.**

In order to perform the test, let's calculate the $p$-value corresponding to the observations. This $p$-value will be calculated manually by using the Pearson's correlation.

```{r lab3_sol, echo=TRUE}
#parameters and correlation
r <- cor(x=Instagram, y=Twitter, method = "pearson")
n <- length(Instagram)
alpha <- 0.05

#t statistic and p-value
t <- r*sqrt((n-2)/(1-r*r))
pvalue <- pt(t, df=n-2, lower.tail = TRUE)

pvalue
```

The resulting $p$-value is greater than $0.05$, so null hypothesis, in this case $\rho=0$, should be accepted. It means that in terms of the test, there is no sufficient significance to assume any association between the number of followers on Twitter and Instagram. Note that it does not implies that this correlation does not exist, it simply establish the fact that, according to the test, it cannot be confirmed that it exists any correlation, also if it is the case.

The situation can also be shown on a graph:

```{r lab3_plot, echo=TRUE}
#plot
library(RColorBrewer)
plotclr <- brewer.pal(6,"YlOrRd")
curve(dt(x,n-2),xlim=c(-5,5), ylim=c(0,0.4),
      main="p-values and rejection region", col = "blue", lwd = 2, xlab="Corr(X, Y)",  ylab=expression(t[n-2]),  yaxs="i")
cord.x_r <- c(qt(1-alpha/2,n-2),seq(qt(1-alpha/2,n-2), 5, 0.01), 5)
cord.y_r <- dt(cord.x_r, n-2); cord.y_r[1] <- 0; cord.y_r[length(cord.y_r)] <- 0
cord.x_l <- c(-5, seq(-5, qt(alpha/2,n-2), 0.01), qt(alpha/2,n-2))
cord.y_l <- dt(cord.x_l, n-2); cord.y_l[1] <- 0; cord.y_l[length(cord.y_l)] <- 0
polygon(cord.x_r,cord.y_r,col=plotclr[3], border = NA )
polygon(cord.x_l,cord.y_l,col=plotclr[3], border = NA )
curve(dt(x,n-2),xlim=c(-5,5),main=expression(t[n-2]), col = "blue", lwd = 2, add = TRUE, yaxs="i")
abline(v =t, lty=2, lwd=2, col="red")
text (0,0.2, paste("Accept", expression(H0)))
text (4.3,0.08, paste("Reject", expression(H0)))
text (-4.3,0.08, paste("Reject", expression(H0)))
text(as.double(t)-0.15, 0.02, "t", col="red", cex=1.2)
```

As can be seen, the estimated statistic $T$ is within the acceptance region, as $p$-value indicates.


### Exercise 4

Compute analitically $J(\gamma, \gamma; y)$, $J(\gamma, \beta; y)$, $J(\beta, \beta; y)$.

**Solution.**

Since $J$ matrix must be derived from the log-likelihood, let's remember its expression:

$$
l(\gamma,\beta;y)=n \log(\gamma)  -  n \gamma \log(\beta)  +  \gamma \sum_{i=1}^n(\log(y_i))  -  \sum_{i=1}^n(y_i/\beta)^\gamma
$$
As $J$ matrix is defined by using the second-order derivatives on model's parameters, previously first-order derivatives must be computed. In order to make the computation, it will be consiedered the next identities:

$$
\begin{cases}
\sum_{i=1}^n(y_i/\beta)^\gamma = \beta^{-\gamma}\sum_{i=1}^ny_i^\gamma \ \ \ \ \ \ \mathrm{(a)} \\
\sum_{i=1}^n(y_i/\beta)^\gamma = \sum_{i=1}^ne^{\gamma\log(y_i/\beta)} \ \ \mathrm{(b)}
\end{cases}
$$

In the computation of $\partial_\gamma$ and $\partial_\beta$, (b) and (a) identities are used respectively.

$$
\frac{\partial l(\gamma,\beta;y)}{\partial \gamma}  =  \frac{n}{\gamma}  -  n\log(\beta)  +  \sum_{i=1}^n\log(y_i/\beta)^\gamma   \\
\frac{\partial l(\gamma,\beta;y)}{\partial \beta} = \frac{n\gamma}{\beta}  -  \frac{\gamma}{\beta} \sum_{i=1}^ny^i/\beta
$$
Now it is possible to compute second-order derivatives, which constitutes the elements of the $J$ matrix. Again to compute this expressions the (a) and (b) identities are used.

1. $\partial_{\gamma\gamma}^2$ term:

$$
\frac{\partial^2 l(\gamma,\beta;y)}{\partial\gamma^2}  =  -\frac{n}{\gamma^2}  +  \sum_{i=1}^n \left( \frac{y_i}{\beta} \right)^\gamma \log\left( \frac{y_i}{\beta} \right)^2
$$
2. $\partial_{\gamma\beta}^2$ term:

$$
\frac{\partial^2 l(\gamma,\beta;y)}{\partial\gamma\partial\beta}  =  \frac{1}{\beta} \left[ n  +  \sum_{i=1}^n \left( \frac{y_i}{\beta} \right)^\gamma  +  \gamma\sum_{i=1}^n \left( \frac{y_i}{\beta} \right)^\gamma \log \frac{y_i}{\beta} \right]
$$
3. $\partial_{\beta\gamma}^2$ term:

$$
\frac{\partial^2 l(\gamma,\beta;y)}{\partial\beta\partial\gamma}  =  -\frac{n}{\beta}  +  \sum_{i=1}^n \left( \frac{y_i}{\beta} \right)^{\gamma-1}  -  \frac{\gamma}{\beta}\sum_{i=1}^n \left( \frac{y_i}{\beta} \right)^\gamma \log \frac{y_i}{\beta}
$$
4. $\partial_{\beta\beta}^2$ term:

$$
\frac{\partial^2 l(\gamma,\beta;y)}{\partial\beta^2}  =  -\frac{n\gamma}{\beta^2}  +  \gamma(\gamma+1)\beta^{-\gamma-2} \sum_{i=1}^ny_i^\gamma
$$


### Exercise 5

Produce the contour plot for the quadratic approximation of the log-likelihood, based on the Taylor series:

$$
\mathcal{l}(\theta) − \mathcal{l}(\widehat{\theta}) \approx - \frac12 (\theta - \widehat{\theta})^T J(\widehat{\theta})(\theta - \widehat{\theta}).
$$

**Solution.**

We then have that

$$
\mathcal{l}(\theta) \approx \mathcal{l}(\widehat{\theta}) - \frac12 (\theta - \widehat{\theta})^T J(\widehat{\theta})(\theta - \widehat{\theta}).
$$

We write the log-likelihood function in $\texttt{R}$:

```{r, echo=TRUE}
log_lik_weibull <- function(data, param){
  sum(dweibull(data, shape = param[1], scale = param[2], log = TRUE))
}
```

Then we write the quadratic approximation of the log-likelihood function in $\texttt{R}$:

```{r}
qapprox_llweibull <- function(data, param, paramhat, Jhat){
  diff <- cbind(param[1] - paramhat[1], param[2] - paramhat[2])
  log_lik_weibull(data, paramhat) - 0.5 * diff %*% Jhat %*% t(diff)
}
```

Finally, we show the contour plot:

```{r, echo=TRUE}
y <- c(155.9, 200.2, 143.8, 150.1,152.1, 142.2, 147, 146, 146, 170.3, 148, 140, 118, 144, 97)
n <- length(y)

gammahat<-uniroot(function(x) n/x+sum(log(y))-n*sum(y^x*log(y))/sum(y^x), c(1e-5,15))$root
betahat<- mean(y^gammahat)^(1/gammahat)
weib.y.mle<-c(gammahat,betahat) #first element is the MLE for the shape gamma,
# while second element is the MLE for the scale beta

#observed information matrix
jhat<-matrix(NA,nrow=2,ncol=2)
jhat[1,1]<-n/gammahat^2+sum((y/betahat)^gammahat* (log(y/betahat))^2)
jhat[1,2]<-jhat[2,1]<- n/betahat-sum(y^gammahat/betahat^(gammahat+1)*(gammahat*log(y/betahat)+1))
jhat[2,2]<- -n*gammahat/betahat^2+gammahat*(gammahat+1) / betahat^(gammahat+2)*sum(y^gammahat)

#define parameters grid
gamma <- seq(0.1, 15, length=100)
beta <- seq(100,200, length=100)
parvalues <- expand.grid(gamma,beta)
appllikvalues <- apply(parvalues, 1, qapprox_llweibull, data=y,
                    paramhat=weib.y.mle, Jhat=jhat)
appllikvalues <- matrix(appllikvalues, nrow=length(gamma),
                     ncol=length(beta), byrow=F)
conf.levels <- c(0,0.5,0.75,0.9,0.95,0.99)

#contour plot
contour(gamma, beta, appllikvalues - max(appllikvalues),
    levels=-qchisq(conf.levels, 2)/2,
    xlab=expression(gamma),
    labels=as.character(conf.levels),
    ylab=expression(beta))
title('Weibull quadratic approximation of log likelihood')

#image
image(gamma, beta, appllikvalues - max(appllikvalues), zlim=c(-6,0),
    col=terrain.colors(20),xlab=expression(gamma),
    ylab=expression(beta))
title('Weibull quadratic approximation of log likelihood')
```

<!-- knitr::knit("Homework1_2020_GROUP_I.Rmd", tangle = TRUE, output ="Homework1_2020_GROUP_I.R") -->